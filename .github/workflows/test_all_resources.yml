name: Test Generate MikroTik RSC Files for scripts
on:
  schedule:
    - cron: "10 0 * * *" # 00:10 UTC
  workflow_dispatch:
permissions:
  contents: write
env:
  JSON_DIR_URL: https://api.github.com/repos/MetaCubeX/meta-rules-dat/contents/sing/geo/geosite
  MAX_ENTRIES_PER_FILE: 150
  GROUP: "torrent, \
    anime, \
    porn, \
    news, \
    games, \
    education, \
    casino, \
    music, \
    video, \
    art"
  SITE: "basecamp.com, \
    bestchange.ru, \
    canva.com, \
    claude.ai, \
    copilot, \
    deepl.com, \
    elevenlabs.io, \
    notepad-plus-plus.org, \
    sentry.io, \
    strava.com, \
    daramalive.life, \
    doramy.club, \
    filmix.fm, \
    hdrezka.ag, \
    kinobase.org, \
    kinovod.pro, \
    kinozal.tv, \
    lostfilm.tv, \
    zeflix.online, \
    soundcloud.com, \
    signal.org"
  GROUP_URL_TEMPLATE: https://iplist.opencck.org/?format=json&data=domains&wildcard=1&group={group}
  SITE_URL_TEMPLATE: https://iplist.opencck.org/?format=json&data=domains&wildcard=1&site={site}
jobs:
  generate-rsc:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install jq and curl
        run: sudo apt-get update && sudo apt-get install -y jq curl

      - name: Get list of all geosite JSON files from MetaCubeX
        id: get_files
        run: |
          echo "Fetching file list from MetaCubeX/meta-rules-dat..."
          FILES_JSON=$(curl -s -H "Accept: application/vnd.github.v3+json" "$JSON_DIR_URL")
          echo "$FILES_JSON" | jq -r '.[] | select(.name | endswith(".json")) | .name' > geosite_files.txt
          echo "Found $(wc -l < geosite_files.txt) JSON files."

      - name: Process groups and sites form iplist.opencck.org
        run: |
          set -e
          IFS=',' read -ra group_array <<< "${{ env.GROUP }}"
          IFS=',' read -ra site_array <<< "${{ env.SITE }}"
          GROUP_URL_TEMPLATE="${{ env.GROUP_URL_TEMPLATE }}"
          SITE_URL_TEMPLATE="${{ env.SITE_URL_TEMPLATE }}"
          MAX_ENTRIES_PER_FILE="${{ env.MAX_ENTRIES_PER_FILE }}"

          mkdir -p for_scripts

          # Process groups
          for group in "${group_array[@]}"; do
              group=$(echo "$group" | xargs)
              echo "Processing group: $group"
              json_url="${GROUP_URL_TEMPLATE//\{group\}/$group}"
              
              if ! curl -s -f "$json_url" > "input_$group.json"; then
                  echo "Failed to fetch JSON for group $group, skipping..."
                  continue
              fi

              # Initialize array for all suffixes in the group
              group_suffixes=()

              # Get resource names (keys) from JSON
              resources=$(jq -r 'keys[]' "input_$group.json" | sort -u)

              # Process each resource in the group
              while IFS= read -r resource; do
                  # Extract base resource name without TLD
                  base_resource=$(echo "$resource" | sed -E 's/\.[a-zA-Z]+$//')
                  echo "Processing resource: $base_resource (from $resource)"
                  
                  # Get domains for the resource (all treated as suffixes with match-subdomain=yes)
                  suffixes=$(jq -r --arg resource "$resource" '.[$resource][]' "input_$group.json" | sort -u | grep -v '^$' || true)

                  # Add suffixes to group_suffixes for combined group file
                  if [ -n "$suffixes" ]; then
                      while IFS= read -r suffix; do
                          group_suffixes+=("$suffix")
                      done <<< "$suffixes"
                  fi

                  # Combine suffixes into entries for individual resource, only if non-empty
                  all_entries=()
                  if [ -n "$suffixes" ]; then
                      while IFS= read -r suffix; do
                          all_entries+=("suffix:$suffix")
                      done <<< "$suffixes"
                  fi

                  # Skip if no valid entries
                  entry_count=${#all_entries[@]}
                  if [ $entry_count -eq 0 ]; then
                      echo "No valid entries for $base_resource, skipping..."
                      continue
                  fi

                  # Check if entries fit in one file
                  if [ $entry_count -le $MAX_ENTRIES_PER_FILE ]; then
                      output_file="for_scripts/${base_resource}.rsc"
                      echo ":global AddressList" > "$output_file"
                      echo ":global ForwardTo" >> "$output_file"
                      echo "/ip dns static" >> "$output_file"
                      for entry in "${all_entries[@]}"; do
                          type=$(echo "$entry" | cut -d':' -f1)
                          value=$(echo "$entry" | cut -d':' -f2-)
                          if [ "$type" = "suffix" ]; then
                              echo ":if ([:len [find name=\"$value\"]] = 0) do={ add address-list=\$AddressList forward-to=\$ForwardTo comment=\"$base_resource\" match-subdomain=yes type=FWD name=\"$value\" }" >> "$output_file"
                          fi
                      done
                  else
                      # Split into multiple files
                      part=1
                      entry_index=0
                      while [ $entry_index -lt $entry_count ]; do
                          output_file="for_scripts/${base_resource}_part${part}.rsc"
                          echo ":global AddressList" > "$output_file"
                          echo ":global ForwardTo" >> "$output_file"
                          echo "/ip dns static" >> "$output_file"
                          for ((i=0; i<MAX_ENTRIES_PER_FILE && entry_index<entry_count; i++, entry_index++)); do
                              entry=${all_entries[$entry_index]}
                              type=$(echo "$entry" | cut -d':' -f1)
                              value=$(echo "$entry" | cut -d':' -f2-)
                              if [ "$type" = "suffix" ]; then
                                  echo ":if ([:len [find name=\"$value\"]] = 0) do={ add address-list=\$AddressList forward-to=\$ForwardTo comment=\"$base_resource\" match-subdomain=yes type=FWD name=\"$value\" }" >> "$output_file"
                              fi
                          done
                          part=$((part + 1))
                      done
                  fi
              done <<< "$resources"

              # Create combined group file
              if [ ${#group_suffixes[@]} -gt 0 ]; then
                  echo "Creating combined file for group: $group"
                  all_entries=()
                  # Sort and deduplicate group_suffixes
                  sorted_suffixes=$(printf '%s\n' "${group_suffixes[@]}" | sort -u | grep -v '^$' || true)
                  if [ -n "$sorted_suffixes" ]; then
                      while IFS= read -r suffix; do
                          all_entries+=("suffix:$suffix")
                      done <<< "$sorted_suffixes"
                  fi

                  entry_count=${#all_entries[@]}
                  if [ $entry_count -eq 0 ]; then
                      echo "No valid entries for group $group, skipping combined file..."
                  else
                      if [ $entry_count -le $MAX_ENTRIES_PER_FILE ]; then
                          output_file="for_scripts/$group.rsc"
                          echo ":global AddressList" > "$output_file"
                          echo ":global ForwardTo" >> "$output_file"
                          echo "/ip dns static" >> "$output_file"
                          for entry in "${all_entries[@]}"; do
                              type=$(echo "$entry" | cut -d':' -f1)
                              value=$(echo "$entry" | cut -d':' -f2-)
                              if [ "$type" = "suffix" ]; then
                                  echo ":if ([:len [find name=\"$value\"]] = 0) do={ add address-list=\$AddressList forward-to=\$ForwardTo comment=\"$group\" match-subdomain=yes type=FWD name=\"$value\" }" >> "$output_file"
                              fi
                          done
                      else
                          # Split into multiple files
                          part=1
                          entry_index=0
                          while [ $entry_index -lt $entry_count ]; do
                              output_file="for_scripts/${group}_part${part}.rsc"
                              echo ":global AddressList" > "$output_file"
                              echo ":global ForwardTo" >> "$output_file"
                              echo "/ip dns static" >> "$output_file"
                              for ((i=0; i<MAX_ENTRIES_PER_FILE && entry_index<entry_count; i++, entry_index++)); do
                                  entry=${all_entries[$entry_index]}
                                  type=$(echo "$entry" | cut -d':' -f1)
                                  value=$(echo "$entry" | cut -d':' -f2-)
                                  if [ "$type" = "suffix" ]; then
                                      echo ":if ([:len [find name=\"$value\"]] = 0) do={ add address-list=\$AddressList forward-to=\$ForwardTo comment=\"$group\" match-subdomain=yes type=FWD name=\"$value\" }" >> "$output_file"
                                  fi
                              done
                              part=$((part + 1))
                          done
                      fi
                  fi
              else
                  echo "No valid suffixes for group $group, skipping combined file..."
              fi
          done

          # Process individual sites
          for site in "${site_array[@]}"; do
              site=$(echo "$site" | xargs)
              echo "Processing site: $site"
              json_url="${SITE_URL_TEMPLATE//\{site\}/$site}"
              
              if ! curl -s -f "$json_url" > "input_$site.json"; then
                  echo "Failed to fetch JSON for site $site, skipping..."
                  continue
              fi

              # Extract base resource name without TLD
              base_resource=$(echo "$site" | sed -E 's/\.[a-zA-Z]+$//')
              echo "Processing resource: $base_resource (from $site)"

              # Get domains for the site (all treated as suffixes with match-subdomain=yes)
              suffixes=$(jq -r --arg site "$site" '.[$site][]' "input_$site.json" | sort -u | grep -v '^$' || true)

              # Combine suffixes into entries, only if non-empty
              all_entries=()
              if [ -n "$suffixes" ]; then
                  while IFS= read -r suffix; do
                      all_entries+=("suffix:$suffix")
                  done <<< "$suffixes"
              fi

              # Skip if no valid entries
              entry_count=${#all_entries[@]}
              if [ $entry_count -eq 0 ]; then
                  echo "No valid entries for $base_resource, skipping..."
                  continue
              fi

              # Check if entries fit in one file
              if [ $entry_count -le $MAX_ENTRIES_PER_FILE ]; then
                  output_file="for_scripts/${base_resource}.rsc"
                  echo ":global AddressList" > "$output_file"
                  echo ":global ForwardTo" >> "$output_file"
                  echo "/ip dns static" >> "$output_file"
                  for entry in "${all_entries[@]}"; do
                      type=$(echo "$entry" | cut -d':' -f1)
                      value=$(echo "$entry" | cut -d':' -f2-)
                      if [ "$type" = "suffix" ]; then
                          echo ":if ([:len [find name=\"$value\"]] = 0) do={ add address-list=\$AddressList forward-to=\$ForwardTo comment=\"$base_resource\" match-subdomain=yes type=FWD name=\"$value\" }" >> "$output_file"
                      fi
                  done
              else
                  # Split into multiple files
                  part=1
                  entry_index=0
                  while [ $entry_index -lt $entry_count ]; do
                      output_file="for_scripts/${base_resource}_part${part}.rsc"
                      echo ":global AddressList" > "$output_file"
                      echo ":global ForwardTo" >> "$output_file"
                      echo "/ip dns static" >> "$output_file"
                      for ((i=0; i<MAX_ENTRIES_PER_FILE && entry_index<entry_count; i++, entry_index++)); do
                          entry=${all_entries[$entry_index]}
                          type=$(echo "$entry" | cut -d':' -f1)
                          value=$(echo "$entry" | cut -d':' -f2-)
                          if [ "$type" = "suffix" ]; then
                              echo ":if ([:len [find name=\"$value\"]] = 0) do={ add address-list=\$AddressList forward-to=\$ForwardTo comment=\"$base_resource\" match-subdomain=yes type=FWD name=\"$value\" }" >> "$output_file"
                          fi
                      done
                      part=$((part + 1))
                  done
              fi
          done

      - name: Process ALL geosite JSON files from MetaCubeX
        run: |
          set -e
          MAX_ENTRIES_PER_FILE="${{ env.MAX_ENTRIES_PER_FILE }}"
          RAW_BASE="https://raw.githubusercontent.com/MetaCubeX/meta-rules-dat/refs/heads/sing/geo/geosite"

          # Blacklist of TLDs to exclude
          excluded_tlds="cv|dj|dm|im|kg|ki|li|ml|ms|mv|ne|nr|sm|ad|as|bf|bj|bt|cd|cf|ci|ao|bw|ck|ls|mz|vi|zm|bn|bz|cy|et|fj|gi|kh|mm|na|np|pg|sb|sl|vc|mg|ac|af|ag|ai|bi|bs|cg|cm|cu|dz|ga|gd|gl|gm|gs|gy|ht|je|lc|mp|mu|mw|nu|pn|re|rw|sc|sr|st|sx|sy|tf|tj|tl|tt|vg|vu|wf|yt|do|ec|eg|gh|hn|jm|kw|lb|mt|om|py|tr|ae|al|am|at|bg|ch|id|cn|ve|uk|za|zw|ar|au|bd|br|il|ke|nz|th|tz|de|es|fr|gr|hr|hu|ie|is|it|ng|pl|ro|rs|sa|ua|jo|uz|tm|az|ba|bh|bo|by|ca|qa|vn|uy|ug|tn|sv|sk|si|sg|ee|sn|cl|pt|pr|pk|ph|pe|pa|no|ni|my|mx|mn|mk|md|ma|ly|lv|lu|lt|lk|la|kz|kr|iq|in|hk|gt|ge|fi|cr|cz|dk"

          while IFS= read -r json_file; do
              resource="${json_file%.json}"  # remove .json
              echo "Processing resource: $resource (from $json_file)"
              json_url="$RAW_BASE/$json_file"

              if ! curl -s -f "$json_url" -o "input_$resource.json"; then
                  echo "Failed to fetch $json_url, skipping..."
                  continue
              fi

              # Extract domains, suffixes, regex
              domains=$(jq -r '
                .rules[]
                | if (.domain | type == "string") then [.domain]
                  elif (.domain | type == "array") then .domain
                  else [] end
                | .[]
              ' "input_$resource.json" | sort -u)

              suffixes=$(jq -r '
                .rules[]
                | if (.domain_suffix | type == "string") then [.domain_suffix]
                  elif (.domain_suffix | type == "array") then .domain_suffix
                  else [] end
                | .[]
              ' "input_$resource.json" | sort -u)

              regex_list=$(jq -r '
                .rules[]
                | if (.domain_regex | type == "string") then [.domain_regex]
                  elif (.domain_regex | type == "array") then .domain_regex
                  else [] end
                | .[]
              ' "input_$resource.json" | sort -u)

              # Filter out rare TLDs and empty
              filtered_domains=$(echo "$domains" | grep -Ev "\.($excluded_tlds)$" | grep -v '^$' || true)
              filtered_suffixes=$(echo "$suffixes" | grep -Ev "\.($excluded_tlds)$" | grep -v '^$' || true)
              filtered_regex_list=$(echo "$regex_list" | grep -v '^$' || true)

              # Build entries
              all_entries=()
              if [ -n "$filtered_suffixes" ]; then
                  while IFS= read -r s; do all_entries+=("suffix:$s"); done <<< "$filtered_suffixes"
              fi
              if [ -n "$filtered_domains" ]; then
                  while IFS= read -r d; do all_entries+=("domain:$d"); done <<< "$filtered_domains"
              fi
              if [ -n "$filtered_regex_list" ]; then
                  while IFS= read -r r; do
                      escaped=$(echo "$r" | sed -E 's/\\/\\\\\\/g; s/\$/\\$/g; s/"/\\"/g; s/ /\\_/g; s/\?/\\?/g')
                      all_entries+=("regex:$escaped")
                  done <<< "$filtered_regex_list"
              fi

              entry_count=${#all_entries[@]}
              if [ $entry_count -eq 0 ]; then
                  echo "No valid entries for $resource, skipping..."
                  continue
              fi

              # Output to one or multiple files
              if [ $entry_count -le $MAX_ENTRIES_PER_FILE ]; then
                  output_file="for_scripts/${resource}.rsc"
                  echo ":global AddressList" > "$output_file"
                  echo ":global ForwardTo" >> "$output_file"
                  echo "/ip dns static" >> "$output_file"
                  for entry in "${all_entries[@]}"; do
                      type=$(echo "$entry" | cut -d':' -f1)
                      value=$(echo "$entry" | cut -d':' -f2-)
                      case "$type" in
                          domain)
                              echo ":if ([:len [find name=\"$value\"]] = 0) do={ add address-list=\$AddressList forward-to=\$ForwardTo comment=\"$resource\" type=FWD name=\"$value\" }" >> "$output_file"
                              ;;
                          suffix)
                              echo ":if ([:len [find name=\"$value\"]] = 0) do={ add address-list=\$AddressList forward-to=\$ForwardTo comment=\"$resource\" match-subdomain=yes type=FWD name=\"$value\" }" >> "$output_file"
                              ;;
                          regex)
                              echo ":if ([:len [find regexp=\"$value\"]] = 0) do={ add address-list=\$AddressList forward-to=\$ForwardTo comment=\"$resource\" type=FWD regexp=\"$value\" }" >> "$output_file"
                              ;;
                      esac
                  done
              else
                  part=1
                  idx=0
                  while [ $idx -lt $entry_count ]; do
                      output_file="for_scripts/${resource}_part${part}.rsc"
                      echo ":global AddressList" > "$output_file"
                      echo ":global ForwardTo" >> "$output_file"
                      echo "/ip dns static" >> "$output_file"
                      for ((i=0; i<MAX_ENTRIES_PER_FILE && idx<entry_count; i++, idx++)); do
                          entry=${all_entries[$idx]}
                          type=$(echo "$entry" | cut -d':' -f1)
                          value=$(echo "$entry" | cut -d':' -f2-)
                          case "$type" in
                              domain)
                                  echo ":if ([:len [find name=\"$value\"]] = 0) do={ add address-list=\$AddressList forward-to=\$ForwardTo comment=\"$resource\" type=FWD name=\"$value\" }" >> "$output_file"
                                  ;;
                              suffix)
                                  echo ":if ([:len [find name=\"$value\"]] = 0) do={ add address-list=\$AddressList forward-to=\$ForwardTo comment=\"$resource\" match-subdomain=yes type=FWD name=\"$value\" }" >> "$output_file"
                                  ;;
                              regex)
                                  echo ":if ([:len [find regexp=\"$value\"]] = 0) do={ add address-list=\$AddressList forward-to=\$ForwardTo comment=\"$resource\" type=FWD regexp=\"$value\" }" >> "$output_file"
                                  ;;
                          esac
                      done
                      part=$((part + 1))
                  done
              fi
          done < geosite_files.txt

      - name: Commit and push changes
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add for_scripts/*.rsc
          git commit -m "Auto-update: all geosite RSC + iplist.opencck.org" || exit 0
          git push